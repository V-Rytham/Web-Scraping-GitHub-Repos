{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef08add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def scrape_and_save_topic_info(main_url):\n",
    "    page_doc = get_page(main_url)\n",
    "    topics_data = parse_page(page_doc)    \n",
    "    for i in range(0, len(topics_data['title'])):\n",
    "        repo_df = get_repo_info(topics_data['url'][i])\n",
    "        fname = topics_data['title'][i]\n",
    "        if(os.path.exists(fname)):\n",
    "            print(f\"The file {fname} already exists. Skipping...\")\n",
    "            continue\n",
    "        repo_df.to_csv(f\"{fname}_repos.csv\", index=None)\n",
    "        print(f\"Repository information for '{fname}' is saved as {fname}_repos.csv\")\n",
    "\n",
    "def get_page(url):\n",
    "    # Download the page\n",
    "    response = requests.get(url)\n",
    "    #Check successful response \n",
    "    #parse using beautiful soup is response is successful \n",
    "    if 200 <= response.status_code <= 299:\n",
    "        page_contents = response.text\n",
    "        doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "        return doc\n",
    "    else:\n",
    "        raise Exception(f\"Unable to fetch the given page {url}\")\n",
    "\n",
    "def parse_page(doc):\n",
    "    topic_titles = []\n",
    "    topic_descs = []\n",
    "    topic_urls = []\n",
    "    \n",
    "    title_tags = doc.find_all('p', class_=\"f3 lh-condensed mb-0 mt-1 Link--primary\")\n",
    "    for tag in title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "        \n",
    "    desc_tags = doc.find_all('p', class_=\"f5 color-fg-muted mb-0 mt-1\")\n",
    "    for tag in desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "        \n",
    "    link_tags = doc.find_all('a', class_=\"no-underline flex-1 d-flex flex-column\")\n",
    "    base_url = \"https://github.com/topic/\"\n",
    "    \n",
    "    for title in topic_titles:\n",
    "        topic_urls.append(base_url + title)\n",
    "\n",
    "    topic_dict = {\n",
    "        'title': topic_titles,\n",
    "        'description': topic_descs,\n",
    "        'url': topic_urls\n",
    "    }\n",
    "    \n",
    "    topic_df = pd.DataFrame(topic_dict)\n",
    "    topic_df.to_csv('topics.csv', index=None)\n",
    "    \n",
    "    return topic_dict\n",
    "\n",
    "def get_repo_info(topic_url):\n",
    "    soup = get_page(topic_url)\n",
    "    h3_tags = soup.find_all('h3', class_=\"f3 color-fg-muted text-normal lh-condensed\")\n",
    "    \n",
    "    repo_names = [tag.find_all('a')[1].text.strip() for tag in h3_tags[:10]]\n",
    "    \n",
    "    usernames = [tag.find_all('a')[0].text.strip() for tag in h3_tags[:10]]\n",
    "    \n",
    "    star_tags = soup.find_all('span', {'id': \"repo-stars-counter-star\"})\n",
    "    stars_set = [int(float(tag.text[:-1]) * 1000) if tag.text[-1] == 'k' else int(tag.text) for tag in star_tags[:10]]     \n",
    "    \n",
    "    repo_urls = [f\"https://github.com/topics{tag.find_all('a')[0]['href']}\" for tag in h3_tags[:10]]\n",
    "\n",
    "    repo_dict = {\n",
    "        'repo name': repo_names,\n",
    "        'user name': usernames,\n",
    "        'stars count': stars_set,\n",
    "        'repo url': repo_URLS\n",
    "    }\n",
    "    \n",
    "    repo_df = pd.DataFrame(repo_dict)\n",
    "    repo_df.to_csv('repo.csv', index=None)    \n",
    "    return repo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a12c08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository information for '3D' is saved as 3D_repos.csv\n",
      "Repository information for 'Ajax' is saved as Ajax_repos.csv\n",
      "Repository information for 'Algorithm' is saved as Algorithm_repos.csv\n",
      "Repository information for 'Amp' is saved as Amp_repos.csv\n",
      "Repository information for 'Android' is saved as Android_repos.csv\n",
      "Repository information for 'Angular' is saved as Angular_repos.csv\n",
      "Repository information for 'Ansible' is saved as Ansible_repos.csv\n",
      "Repository information for 'API' is saved as API_repos.csv\n",
      "Repository information for 'Arduino' is saved as Arduino_repos.csv\n",
      "Repository information for 'ASP.NET' is saved as ASP.NET_repos.csv\n",
      "Repository information for 'Atom' is saved as Atom_repos.csv\n",
      "Repository information for 'Awesome Lists' is saved as Awesome Lists_repos.csv\n",
      "Repository information for 'Amazon Web Services' is saved as Amazon Web Services_repos.csv\n",
      "Repository information for 'Azure' is saved as Azure_repos.csv\n",
      "Repository information for 'Babel' is saved as Babel_repos.csv\n",
      "Repository information for 'Bash' is saved as Bash_repos.csv\n",
      "Repository information for 'Bitcoin' is saved as Bitcoin_repos.csv\n",
      "Repository information for 'Bootstrap' is saved as Bootstrap_repos.csv\n",
      "Repository information for 'Bot' is saved as Bot_repos.csv\n",
      "Repository information for 'C' is saved as C_repos.csv\n",
      "Repository information for 'Chrome' is saved as Chrome_repos.csv\n",
      "Repository information for 'Chrome extension' is saved as Chrome extension_repos.csv\n",
      "Repository information for 'Command line interface' is saved as Command line interface_repos.csv\n",
      "Repository information for 'Clojure' is saved as Clojure_repos.csv\n",
      "Repository information for 'Code quality' is saved as Code quality_repos.csv\n",
      "Repository information for 'Code review' is saved as Code review_repos.csv\n",
      "Repository information for 'Compiler' is saved as Compiler_repos.csv\n",
      "Repository information for 'Continuous integration' is saved as Continuous integration_repos.csv\n",
      "Repository information for 'COVID-19' is saved as COVID-19_repos.csv\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to fetch the given page https://github.com/topic/C++",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scrape_and_save_topic_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/topics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m, in \u001b[0;36mscrape_and_save_topic_info\u001b[1;34m(main_url)\u001b[0m\n\u001b[0;32m      8\u001b[0m topics_data \u001b[38;5;241m=\u001b[39m parse_page(page_doc)    \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(topics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m---> 10\u001b[0m     repo_df \u001b[38;5;241m=\u001b[39m get_repo_info(topics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n\u001b[0;32m     11\u001b[0m     fname \u001b[38;5;241m=\u001b[39m topics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fname)):\n",
      "Cell \u001b[1;32mIn[29], line 61\u001b[0m, in \u001b[0;36mget_repo_info\u001b[1;34m(topic_url)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_repo_info\u001b[39m(topic_url):\n\u001b[1;32m---> 61\u001b[0m     soup \u001b[38;5;241m=\u001b[39m get_page(topic_url)\n\u001b[0;32m     63\u001b[0m     h3_tags \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf3 color-fg-muted text-normal lh-condensed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m     usernames \u001b[38;5;241m=\u001b[39m [tag\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m h3_tags[:\u001b[38;5;241m10\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[29], line 28\u001b[0m, in \u001b[0;36mget_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to fetch the given page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Unable to fetch the given page https://github.com/topic/C++"
     ]
    }
   ],
   "source": [
    "scrape_and_save_topic_info(\"https://github.com/topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7aa35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e619bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
